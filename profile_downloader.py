# profile_downloader.py
import asyncio
import random
import logging
from utils import ensure_dir, download_file
from tqdm.asyncio import tqdm

logger = logging.getLogger(__name__)

# Constants
SCROLL_WAIT = 2000  # ms
PAGE_LOAD_WAIT = 3000  # ms
MAX_SCROLL_ATTEMPTS = 50  # Tr√°nh scroll v√¥ h·∫°n
CONCURRENT_DOWNLOADS = 5  # S·ªë file t·∫£i ƒë·ªìng th·ªùi


async def extract_media(page):
    """Tr√≠ch xu·∫•t t·∫•t c·∫£ media URLs t·ª´ page"""
    media = set()

    try:
        # Extract images
        imgs = await page.query_selector_all("img")
        for img in imgs:
            src = await img.get_attribute("src")
            if src and "scontent" in src:
                media.add(src)

        # Extract videos
        videos = await page.query_selector_all("video")
        for vid in videos:
            src = await vid.get_attribute("src")
            if src:
                media.add(src)

            # Check source tag
            src_tag = await vid.query_selector("source")
            if src_tag:
                s = await src_tag.get_attribute("src")
                if s:
                    media.add(s)

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi extract media: {e}")

    return media


async def download_profile(page, username, amount):
    """Download media t·ª´ Instagram profile"""
    try:
        logger.info(f"üåê M·ªü profile @{username}")

        # Navigate v·ªõi timeout
        try:
            await page.goto(
                f"https://www.instagram.com/{username}/",
                wait_until="domcontentloaded",
                timeout=30000
            )
            await page.wait_for_timeout(PAGE_LOAD_WAIT)
        except Exception as e:
            logger.error(f"‚ùå Kh√¥ng th·ªÉ truy c·∫≠p profile @{username}: {e}")
            return

        # Check n·∫øu profile kh√¥ng t·ªìn t·∫°i
        not_found = await page.query_selector('text="Sorry, this page isn\'t available."')
        if not_found:
            logger.error(f"‚ùå Profile @{username} kh√¥ng t·ªìn t·∫°i ho·∫∑c b·ªã private!")
            return

        ensure_dir(username)

        collected = []
        last_len = 0
        scroll_count = 0

        logger.info("üîé ƒêang qu√©t b√†i ƒëƒÉng...")

        # Scroll ƒë·ªÉ load th√™m posts
        while len(collected) < amount and scroll_count < MAX_SCROLL_ATTEMPTS:
            # Scroll
            await page.evaluate("window.scrollBy(0, document.body.scrollHeight)")
            await page.wait_for_timeout(SCROLL_WAIT)

            # Rate limiting - tr√°nh b·ªã Instagram ban
            await asyncio.sleep(random.uniform(0.3, 0.8))

            # Extract media
            found = await extract_media(page)
            for url in found:
                if url not in collected:
                    collected.append(url)

            # Check n·∫øu kh√¥ng load th√™m ƒë∆∞·ª£c
            if len(collected) == last_len:
                scroll_count += 1
                if scroll_count >= 3:  # Th·ª≠ 3 l·∫ßn kh√¥ng c√≥ g√¨ m·ªõi ‚Üí d·ª´ng
                    logger.info("‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y th√™m media m·ªõi")
                    break
            else:
                scroll_count = 0

            last_len = len(collected)
            logger.info(f"üìä ƒê√£ t√¨m th·∫•y {len(collected)} media...")

        if not collected:
            logger.warning(f"‚ùå Kh√¥ng t√¨m th·∫•y media n√†o t·ª´ @{username}")
            return

        # Limit s·ªë l∆∞·ª£ng download
        to_download = collected[:amount]
        logger.info(f"üì• B·∫Øt ƒë·∫ßu t·∫£i {len(to_download)} media...")

        # Download v·ªõi progress bar v√† concurrent
        success_count = 0

        # Chia nh·ªè th√†nh batches ƒë·ªÉ tr√°nh qu√° t·∫£i
        for i in range(0, len(to_download), CONCURRENT_DOWNLOADS):
            batch = to_download[i:i + CONCURRENT_DOWNLOADS]
            tasks = []

            for idx, url in enumerate(batch, start=i + 1):
                ext = "mp4" if ".mp4" in url else "jpg"
                path = f"{username}/{idx}.{ext}"
                tasks.append(download_file(page, url, path))

            # Download batch
            results = await asyncio.gather(*tasks, return_exceptions=True)

            # Count successes
            for j, result in enumerate(results):
                if result is True:
                    success_count += 1
                    logger.info(f"‚úÖ [{success_count}/{len(to_download)}] ƒê√£ t·∫£i {i + j + 1}.{ext}")
                elif isinstance(result, Exception):
                    logger.warning(f"‚ö†Ô∏è  L·ªói t·∫£i file {i + j + 1}: {result}")

            # Rate limiting gi·ªØa c√°c batches
            await asyncio.sleep(random.uniform(0.5, 1.0))

        logger.info(f"üéâ Ho√†n t·∫•t! ƒê√£ t·∫£i {success_count}/{len(to_download)} media v√†o folder '{username}/'")

    except Exception as e:
        logger.error(f"‚ùå L·ªói nghi√™m tr·ªçng khi download profile: {e}")
        import traceback
        traceback.print_exc()